import os
import numpy as np
import matplotlib.pyplot as plt
import torchvision.models as models
import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

net = models.resnet18(weights=None, num_classes=10)
try:
    net.load_state_dict(torch.load('/content/image_recognition_model.pth', map_location=torch.device('cpu')))
    net.eval()
except FileNotFoundError:
    print("Error: Model file not found. Ensure '/content/image_recognition_model.pth' exists.")
    exit(1)

target_classes_mapping = {
    0: 2,  # airplane -> bird
    1: 0,  # automobile -> airplane
    2: 3,  # bird -> cat
    3: 5,  # cat -> dog
    4: 3,  # deer -> cat
    5: 3,  # dog -> cat
    6: 3,  # frog -> cat
    7: 3,  # horse -> cat
    8: 1,  # ship -> automobile
    9: 0   # truck -> airplane
}

epsilon = 0.1

net.to(device)

def targeted_jsma_attack(model, data, target_class, epsilon=epsilon, num_classes=10):
    data.requires_grad = True
    output = model(data)

    gradients = []
    for i in range(num_classes):
        model.zero_grad()
        output[0, i].backward(retain_graph=True)
        gradients.append(data.grad.data.clone())
        data.grad.data.zero_()
    gradients = torch.stack(gradients)  # Shape: [num_classes, 3, 32, 32]

    target_grad = gradients[target_class]
    saliency_map = target_grad.sign()

    perturbation = epsilon * saliency_map
    perturbed_data = data + perturbation
    perturbed_data = torch.clamp(perturbed_data, 0, 1)

    return perturbed_data

def save_image_with_classification(image_data, classification, filepath):
    image = np.transpose(image_data.squeeze(0).detach().numpy(), (1, 2, 0))
    image = np.clip(image, 0, 1)
    plt.imshow(image)
    plt.title(classification)
    plt.axis('off')
    plt.savefig(filepath)
    plt.close()

def get_node_activation_hook(name, store_dict):
    def hook(model, input, output):
        # Store the entire layer's activation
        store_dict[name] = output.detach().cpu()  # Shape: [batch_size, channels, height, width]
    return hook

def compute_node_vulnerabilities(original_activations, adversarial_activations):
    node_vulnerabilities = {}
    for layer_name in original_activations.keys():
        if layer_name in adversarial_activations:
            original = original_activations[layer_name]
            adversarial = adversarial_activations[layer_name]

            differences = (adversarial - original).norm(2, dim=(0, 2, 3))  # L2 norm per channel
            node_vulnerabilities[layer_name] = differences.tolist()  # Convert tensor to list for readability
    return node_vulnerabilities

def mask_nodes(layer, node_scores, threshold):
    with torch.no_grad():
        for node_index, score in enumerate(node_scores):
            if score > threshold:
                # Zero out weights and biases of the node
                layer.weight[node_index] = 0
                if layer.bias is not None:
                    layer.bias[node_index] = 0

class_names = datasets.CIFAR10(root='./data', train=True, download=True).classes

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
test_loader = DataLoader(test_data, batch_size=1, shuffle=True)

num_images = 300
output_dir = './output_images'
os.makedirs(output_dir, exist_ok=True)
threshold_fixed = 80  # Set fixed threshold for masking

all_labels = []
all_original_preds = []
all_adversarial_preds = []

target_class_success = 0  # Tracks how many perturbed images are classified as the target class
correct_original = 0
correct_adversarial = 0

def register_hooks(model, activation_store):
    hooks = []
    for name, layer in model.named_modules():
        if isinstance(layer, (nn.ReLU, nn.Conv2d)):
            hooks.append(layer.register_forward_hook(get_node_activation_hook(name, activation_store)))
    return hooks

num_perturbed_images = 0
nodes_to_mask = [21, 11, 42, 19]  # List to store indices of nodes to be masked for 12.5% --> 55, 32, 10, 53 ---------- for 75% --> , 2, 1, 49, 57, 14, 52, 13, 56, 4, 45, 38, 6, 33, 31, 47, 23, 16, 28, 62, 15, 36, 39, 12, 20, 9, 35, 29, 34, 48, 63, 17, 0, 24, 43, 26, 61, 25, 3, 8, 59

for i, (data, target) in enumerate(test_loader):
    if i >= num_images:
        break

    data, target = data.to(device), target.to(device)
    all_labels.append(target.item())
    #original_pred = net(data).argmax(dim=1).item()
    #all_original_preds.append(original_pred)

    original_activations = {}
    adversarial_activations = {}
    original_hooks = register_hooks(net, original_activations)
    adversarial_hooks = register_hooks(net, adversarial_activations)

    with torch.no_grad():
        output = net(data)
    original_pred = output.argmax(dim=1).item()
    all_original_preds.append(original_pred)
    correct_original += (original_pred == target.item())

    for hook in original_hooks:
        hook.remove()

    target_class = target_classes_mapping.get(target.item(), target.item())
    adversarial_data = targeted_jsma_attack(net, data.clone(), target_class=target_class, epsilon=epsilon)

    num_perturbed_images += 1

    with torch.no_grad():
        output = net(adversarial_data)
    adversarial_pred = output.argmax(dim=1).item()
    correct_adversarial += (adversarial_pred == target_class)

    for hook in adversarial_hooks:
        hook.remove()

    node_vulnerabilities = compute_node_vulnerabilities(original_activations, adversarial_activations)

    # Identify nodes to mask in layer1.1.conv1
   # if "layer1.1.conv1" in node_vulnerabilities:
   #     for node_index, score in enumerate(node_vulnerabilities["layer1.1.conv1"]):
   #        if score > threshold_fixed:
    #            nodes_to_mask.append(node_index)

accuracy_original_before_masking = (correct_original / num_images) * 100
accuracy_adversarial_before_masking = (correct_adversarial / num_perturbed_images) * 100

print(f"\nAccuracy on original images before masking: {accuracy_original_before_masking:.2f}%")
print(f"Accuracy on adversarial images before masking: {accuracy_adversarial_before_masking:.2f}%")

print("\nEvaluating the model after masking nodes...\n")

correct_original_after_masking = 0
correct_adversarial_after_masking = 0

nodes_to_mask = list(set(nodes_to_mask))
print(f"Nodes to be masked in layer1.1.conv1: {nodes_to_mask}")

target_layer = net.layer1[1].conv1
mask_nodes(target_layer, [threshold_fixed + 1 if i in nodes_to_mask else 0 for i in range(target_layer.weight.size(0))], threshold_fixed)

masked_model_path = './masked_model.pth'
torch.save(net.state_dict(), masked_model_path)
print(f"Masked model saved to {masked_model_path}")

for i, (data, target) in enumerate(test_loader):
    if i >= num_images:
        break

    data, target = data.to(device), target.to(device)
    all_labels.append(target.item())

    with torch.no_grad():
        output = net(data)
    original_pred = output.argmax(dim=1).item()
    correct_original_after_masking += (original_pred == target.item())

    target_class = target_classes_mapping.get(target.item(), target.item())
    adversarial_data = targeted_jsma_attack(net, data.clone(), target_class=target_class, epsilon=epsilon)

    with torch.no_grad():
        output = net(adversarial_data)
    adversarial_pred = output.argmax(dim=1).item()
    correct_adversarial_after_masking += (adversarial_pred == target_class)

accuracy_original_after_masking = correct_original_after_masking / num_images * 100
accuracy_adversarial_after_masking = correct_adversarial_after_masking / num_perturbed_images * 100

print(f"\nAccuracy on original images after masking: {accuracy_original_after_masking:.2f}%")
print(f"Accuracy on adversarial images after masking: {accuracy_adversarial_after_masking:.2f}%")

node_vulnerabilities = compute_node_vulnerabilities(original_activations, adversarial_activations)

output_file_path = './node_vulnerabilities.txt'
with open(output_file_path, 'w') as file:
    for layer_name, node_scores in node_vulnerabilities.items():
        file.write(f"Node vulnerabilities for {layer_name}:\n")
        for node_index, score in enumerate(node_scores):
            file.write(f"  Node {node_index}: {score:.4f}\n")
        file.write("\n")
print(f"Node vulnerabilities saved to {output_file_path}")
