import os
import numpy as np
import matplotlib.pyplot as plt
import torchvision.models as models
import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

net = models.resnet18(weights=None, num_classes=10)
try:
    net.load_state_dict(torch.load('/content/image_recognition_model.pth', map_location=torch.device('cpu')))
    net.eval()
except FileNotFoundError:
    print("Error: Model file not found. Ensure '/content/image_recognition_model.pth' exists.")
    exit(1)
target_classes_mapping = {
    0: 2,  # airplane -> bird
    1: 0,  # automobile -> airplane
    2: 3,  # bird -> cat
    3: 5,  # cat -> dog
    4: 3,  # deer -> cat
    5: 3,  # dog -> cat
    6: 3,  # frog -> cat
    7: 3,  # horse -> cat
    8: 1,  # ship -> automobile
    9: 0   # truck -> airplane
}

epsilon = 0.1

def targeted_jsma_attack(model, data, target_class, epsilon=epsilon, num_classes=10):
    data.requires_grad = True
    output = model(data)

    gradients = []
    for i in range(num_classes):
        model.zero_grad()
        output[0, i].backward(retain_graph=True)
        gradients.append(data.grad.data.clone())
        data.grad.data.zero_()
    gradients = torch.stack(gradients)  # Shape: [num_classes, 3, 32, 32]

    target_grad = gradients[target_class]
    saliency_map = target_grad.sign()

    perturbation = epsilon * saliency_map
    perturbed_data = data + perturbation
    perturbed_data = torch.clamp(perturbed_data, 0, 1)

    return perturbed_data

def save_image_with_classification(image_data, classification, filepath):
    image = np.transpose(image_data.squeeze(0).detach().numpy(), (1, 2, 0))
    image = np.clip(image, 0, 1)
    plt.imshow(image)
    plt.title(classification)
    plt.axis('off')
    plt.savefig(filepath)
    plt.close()

def get_node_activation_hook(name, store_dict):
    def hook(model, input, output):
        # Store the entire layer's activation
        store_dict[name] = output.detach().cpu()  # Shape: [batch_size, channels, height, width]
    return hook

def compute_node_vulnerabilities(original_activations, adversarial_activations):
    node_vulnerabilities = {}
    for layer_name in original_activations.keys():
        if layer_name in adversarial_activations:
            original = original_activations[layer_name]
            adversarial = adversarial_activations[layer_name]

            differences = (adversarial - original).norm(2, dim=(0, 2, 3))  # L2 norm per channel
            node_vulnerabilities[layer_name] = differences.tolist()  # Convert tensor to list for readability
    return node_vulnerabilities

class_names = datasets.CIFAR10(root='./data', train=True, download=True).classes

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
test_loader = DataLoader(test_data, batch_size=1, shuffle=True)

num_images = 300
output_dir = './output_images'
os.makedirs(output_dir, exist_ok=True)

all_labels = []
all_original_preds = []
all_adversarial_preds = []

target_class_success = 0  # Tracks how many perturbed images are classified as the target class
correct_original = 0
correct_adversarial = 0

def register_hooks(model, activation_store):
    hooks = []
    for name, layer in model.named_modules():
        if isinstance(layer, (nn.ReLU, nn.Conv2d)):
            hooks.append(layer.register_forward_hook(get_node_activation_hook(name, activation_store)))
    return hooks

num_perturbed_images = 0

for i, (data, target) in enumerate(test_loader):
    if i >= num_images:
        break

    data, target = data.to('cpu'), target.to('cpu')
    all_labels.append(target.item())

    original_activations = {}
    original_hooks = register_hooks(net, original_activations)

    with torch.no_grad():
        output = net(data)
    original_pred = output.argmax(dim=1).item()
    all_original_preds.append(original_pred)
    correct_original += (original_pred == target.item())

    for hook in original_hooks:
        hook.remove()

    target_class = target_classes_mapping.get(target.item(), target.item())

    adversarial_activations = {}
    adversarial_hooks = register_hooks(net, adversarial_activations)

    adversarial_data = targeted_jsma_attack(net, data.clone(), target_class=target_class, epsilon=epsilon)

    num_perturbed_images += 1

    with torch.no_grad():
        output = net(adversarial_data)
    adversarial_pred = output.argmax(dim=1).item()
    if adversarial_pred == target_class:  # Check for success
        target_class_success += 1

    for hook in adversarial_hooks:
        hook.remove()

node_vulnerabilities = compute_node_vulnerabilities(original_activations, adversarial_activations)

output_file_path = './node_vulnerabilities.txt'
with open(output_file_path, 'w') as file:
    for layer_name, node_scores in node_vulnerabilities.items():
        file.write(f"Node vulnerabilities for {layer_name}:\n")
        print(f"Node vulnerabilities for {layer_name}:")
        for node_index, score in enumerate(node_scores):
            file.write(f"  Node {node_index}: {score:.4f}\n")
            print(f"  Node {node_index}: {score:.4f}")
        file.write("\n")
print(f"Node vulnerabilities saved to {output_file_path}")
