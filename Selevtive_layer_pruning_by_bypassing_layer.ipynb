#enhanced pruned model with idenity layer and fine tuning for higher layers
import os
import numpy as np
import matplotlib.pyplot as plt
import torchvision.models as models
import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torchvision.utils import save_image

# Check for GPU availability
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load CIFAR-10 dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])
dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

# CIFAR-10 class mapping to closest classes
target_classes_mapping = {
    0: 2,  # airplane -> bird
    1: 0,  # automobile -> airplane
    2: 3,  # bird -> cat
    3: 5,  # cat -> dog
    4: 3,  # deer -> cat
    5: 3,  # dog -> cat
    6: 3,  # frog -> cat
    7: 3,  # horse -> cat
    8: 1,  # ship -> automobile
    9: 0   # truck -> airplane
}

# Load pre-trained ResNet-18 model
net = models.resnet18(weights=None, num_classes=10)
try:
    # Load saved weights
    net.load_state_dict(torch.load('/content/image_recognition_model.pth', map_location=device))
    net.eval()  # Set to evaluation mode
    print("Model loaded successfully and set to evaluation mode.")
except FileNotFoundError:
    print("Error: Model file not found. Ensure '/content/image_recognition_model.pth' exists.")
    exit(1)
except RuntimeError as e:
    print(f"Error loading model: {e}")
    exit(1)

# Move the model to the selected device
net.to(device)

# Bypass strategy for pruning
class ModifiedResNet18(nn.Module):
    def __init__(self, original_model):
        super(ModifiedResNet18, self).__init__()
        self.features = nn.ModuleDict(original_model.named_children())
        self.features['layer1'][1].conv1 = nn.Identity()  # Replace layer1.1.conv1 with Identity

    def forward(self, x):
        x = self.features['conv1'](x)
        x = self.features['bn1'](x)
        x = self.features['relu'](x)
        x = self.features['maxpool'](x)
        x = self.features['layer1'](x)
        x = self.features['layer2'](x)
        x = self.features['layer3'](x)
        x = self.features['layer4'](x)
        x = self.features['avgpool'](x)
        x = torch.flatten(x, 1)
        x = self.features['fc'](x)
        return x

# Create the pruned model
pruned_model = ModifiedResNet18(net).to(device)

# Save the modified model for testing
torch.save(pruned_model.state_dict(), './pruned_resnet18.pth')
print("Pruned model saved successfully.")

# Prepare folders for saving images
os.makedirs('./images/original', exist_ok=True)
os.makedirs('./images/perturbed', exist_ok=True)

# Save images with predictions and classifications
def save_image_with_predictions(image_data, filepath, original_label, prediction):
    """
    Save an image with its predicted and original labels in the title.
    """
    image = np.transpose(image_data.squeeze(0).detach().cpu().numpy(), (1, 2, 0))  # Ensure data is on CPU
    image = np.clip(image, 0, 1)
    plt.imshow(image)
    plt.title(f"Original: {original_label}, Predicted: {prediction}")
    plt.axis('off')
    plt.savefig(filepath)
    plt.close()

# Parameters
epsilon = 0.1

# JSMA attack implementation
def targeted_jsma_attack(model, data, target_class, epsilon=epsilon, num_classes=10):
    data.requires_grad = True
    output = model(data)

    # Compute the Jacobian of the output with respect to the input
    gradients = []
    for i in range(num_classes):
        model.zero_grad()
        output[0, i].backward(retain_graph=True)
        gradients.append(data.grad.data.clone())
        data.grad.data.zero_()
    gradients = torch.stack(gradients)  # Shape: [num_classes, 3, 32, 32]

    # Focus the saliency map on the target class
    target_grad = gradients[target_class]
    saliency_map = target_grad.sign()

    # Perturb the input image
    perturbation = epsilon * saliency_map
    perturbed_data = data + perturbation
    perturbed_data = torch.clamp(perturbed_data, 0, 1)

    return perturbed_data

# CIFAR-10 class names
class_names = datasets.CIFAR10(root='./data', train=True, download=True).classes

# Initialize tracking variables
num_images = 300
num_perturbed_images = 0
correct_original = 0
correct_adversarial = 0
target_class_success = 0

# Process images
for i, (data, target) in enumerate(dataloader):
    if i >= num_images:  # Limit to 300 images
        break

    data, target = data.to(device), target.to(device)  # Move data to GPU/CPU
    original_class = target.item()
    target_class = target_classes_mapping.get(original_class, original_class)  # Get mapped target class

    # Original prediction
    with torch.no_grad():
        output = pruned_model(data)
    original_pred = output.argmax(dim=1).item()
    correct_original += (original_pred == target.item())

      # Save original image with predictions
    save_image_with_predictions(
        data,
        f'./images/original/class_{original_class}_img_{i}.png',
        class_names[original_class],
        class_names[original_pred]
    )

    # Generate perturbed image
    perturbed_data = targeted_jsma_attack(pruned_model, data.clone(), target_class)

    # Adversarial prediction
    with torch.no_grad():
        output = pruned_model(perturbed_data)
    adversarial_pred = output.argmax(dim=1).item()

    num_perturbed_images += 1
    if adversarial_pred == target_class:  # Check if target class was successfully achieved
        target_class_success += 1
    if adversarial_pred == target.item():
        correct_adversarial += 1

     # Save perturbed image with predictions
    save_image_with_predictions(
        perturbed_data,
        f'./images/perturbed/class_{original_class}_to_{target_class}_img_{i}_perturbed.png',
        class_names[original_class],
        class_names[adversarial_pred]
    )

# Calculate metrics
accuracy_original = correct_original / num_images
accuracy_adversarial = correct_adversarial / num_images
deceive_rate = 100 - (correct_adversarial / num_perturbed_images * 100)
tcsr = target_class_success / num_perturbed_images * 100

print(f"\nNumber of perturbed images: {num_perturbed_images}")
print(f"\nAccuracy on original images: {accuracy_original * 100:.2f}%")
print(f"\nAccuracy on perturbed images: {accuracy_adversarial * 100:.2f}%")
print(f"\nDeceive rate of the perturbation: {deceive_rate:.2f}%")
print(f"\nTarget Class Success Rate (TCSR): {tcsr:.2f}%")

