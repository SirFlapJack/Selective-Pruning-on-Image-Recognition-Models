import os
import numpy as np
import matplotlib.pyplot as plt
import torchvision.models as models
import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader


# Load the model (pre-trained ResNet-18)
net = models.resnet18(weights=None, num_classes=10)
try:
    net.load_state_dict(torch.load('/content/masked_model.pth', map_location=torch.device('cpu'))) #/content/image_recognition_model.pth
    net.eval()
except FileNotFoundError:
    print("Error: Model file not found. Ensure '/content/masked_model.pth' exists.") #/content/image_recognition_model.pth
    exit(1)

# Variables for dynamic pruning
ImageIsPerturbed = True  # Default: no perturbation
PerturbationClass = 'JSMA'  # Default: no perturbation class
DynamicPruningAccordingToPerturbationMethod = {
    'JSMA': ['layer1.1.conv1', 'layer1.0.conv1', 'layer3.1.conv1', 'layer2.0.conv1'],  # Example layers for JSMA
    'FGSM': []  # To be filled later
    }

epsilon = 0.1

# JSMA attack implementation
def targeted_jsma_attack(model, data, target_class, epsilon=epsilon, num_classes=10):
    data.requires_grad = True
    output = model(data)

    # Compute the Jacobian of the output with respect to the input
    gradients = []
    for i in range(num_classes):
        model.zero_grad()
        output[0, i].backward(retain_graph=True)
        gradients.append(data.grad.data.clone())
        data.grad.data.zero_()
    gradients = torch.stack(gradients)  # Shape: [num_classes, 3, 32, 32]

    # Focus the saliency map on the target class
    target_grad = gradients[target_class]
    saliency_map = target_grad.sign()

    # Perturb the input image
    perturbation = epsilon * saliency_map
    perturbed_data = data + perturbation
    perturbed_data = torch.clamp(perturbed_data, 0, 1)

    return perturbed_data


# Save images with classifications
def save_image_with_classification(image_data, classification, filepath):
    image = np.transpose(image_data.squeeze(0).detach().numpy(), (1, 2, 0))
    image = np.clip(image, 0, 1)
    plt.imshow(image)
    plt.title(classification)
    plt.axis('off')
    plt.savefig(filepath)
    plt.close()

# Hook for activations
def get_activation_hook(name, store_dict):
    def hook(model, input, output):
        store_dict[name] = output.detach().cpu()
    return hook

# CIFAR-10 class names
class_names = datasets.CIFAR10(root='./data', train=True, download=True).classes

# Data transformation and loading
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
test_loader = DataLoader(test_data, batch_size=1, shuffle=True)

# Parameters
#epsilon = 0.1
num_images = 300
output_dir = './output_images'
os.makedirs(output_dir, exist_ok=True)

# Lists to collect results
all_labels = []
all_original_preds = []
all_adversarial_preds = []

target_class_success = 0  # Tracks how many perturbed images are classified as the target class

correct_original = 0
correct_adversarial = 0

# Helper function to register hooks
def register_hooks(model, activation_store):
    hooks = []
    for name, layer in model.named_modules():
        if isinstance(layer, (nn.ReLU, nn.Conv2d)):
            hooks.append(layer.register_forward_hook(get_activation_hook(name, activation_store)))
    return hooks

# Initialize variable to count the number of perturbed images created
num_perturbed_images = 0

# Process images
for i, (data, target) in enumerate(test_loader):
    if i >= num_images:
        break

    data, target = data.to('cpu'), target.to('cpu')
    all_labels.append(target.item())

    # Original activations
    original_activations = {}
    original_hooks = register_hooks(net, original_activations)

    # Original prediction
    with torch.no_grad():
        output = net(data)
    original_pred = output.argmax(dim=1).item()
    all_original_preds.append(original_pred)
    correct_original += (original_pred == target.item())

    # Remove original hooks
    for hook in original_hooks:
        hook.remove()

# 0 - airplane
# 1 - automobile
# 2 - bird
# 3 - cat
# 4 - deer
# 5 - dog
# 6 - frog
# 7 - horse
# 8 - ship
# 9 - truck

   # Adversarial example generation for specific class (e.g., airplane)
    if target.item() == 9:  # Attack only class "truck"
        adversarial_activations = {}
        adversarial_hooks = register_hooks(net, adversarial_activations)

        adversarial_data = targeted_jsma_attack(net, data.clone(), target_class=0, epsilon=epsilon)  # Target class = airplane

        # Increment the number of perturbed images
        num_perturbed_images += 1

        # Adversarial prediction
        with torch.no_grad():
            output = net(adversarial_data)
        adversarial_pred = output.argmax(dim=1).item()
        #all_adversarial_preds.append(adversarial_pred)
        if adversarial_pred == 0:  # Check for success
            target_class_success += 1

        # Log perturbation magnitude
        perturbation_magnitude = (adversarial_data - data).norm().item()
        print(f"Perturbation magnitude (L2 norm): {perturbation_magnitude:.4f}")

        # Check if the prediction matches the original class
        if adversarial_pred == target.item():
            correct_adversarial += 1
            print(f"Correctly classified perturbed image as {class_names[target.item()]}")
        else:
            print(f"Misclassified perturbed image as {class_names[adversarial_pred]}")

        # Save images
        save_image_with_classification(
            adversarial_data,
            f'Original={class_names[target.item()]}, Predicted={class_names[adversarial_pred]}',
            f'{output_dir}/image{i}_perturbed.png'
        )

        # Remove adversarial hooks  <-- Moved this loop inside the conditional block
        for hook in adversarial_hooks:
            hook.remove()

#print the number of images perturbed
print(f"\nNumber of perturbed images: {num_perturbed_images}")

#print the acc on the perturbed images based on the perturbed image count
print(f"\nAccuracy on perturbed images: {correct_adversarial / num_perturbed_images * 100:.2f}%")

#print the deceived accuracy based on the what is left based on the accuracy on perturbed images
print(f"\nDeceive rate of the perturbation (wrongly classified perturbed images): {100-correct_adversarial / num_perturbed_images * 100:.2f}%")

# Calculate Target Class Success Rate (TCSR)
tcsr = target_class_success / num_perturbed_images * 100
print(f"\nTarget Class Success Rate (TCSR) (Confusion rate to the desired target class): {tcsr:.2f}%")

# Calculate accuracy
accuracy_original = correct_original / num_images
accuracy_adversarial = correct_adversarial / num_images

print(f'\nAccuracy on original images: {accuracy_original * 100:.2f}%')
print(f'Accuracy on adversarial images: {accuracy_adversarial * 100:.2f}%')

# Compute layer vulnerabilities
layer_vulnerabilities = {
    name: (adversarial_activations[name] - original_activations[name]).norm(2).item()
    for name in original_activations.keys() if name in adversarial_activations
}

sorted_layers = sorted(layer_vulnerabilities.items(), key=lambda x: x[1], reverse=True)
print("\nLayer Vulnerabilities (most affected):")
for layer, vulnerability in sorted_layers:
    print(f"{layer}: {vulnerability:.4f}")
